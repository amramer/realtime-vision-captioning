{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iiuJN95zE8W"
   },
   "source": [
    "# Realtime Webcam Caption + Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a926b705aedf410f80dd3942cc974c73",
      "feb69592acf14127a22fa07eef055936",
      "d9a31e91f624414dad9b6152fbdb349c",
      "0469a71cadaa4a85b2b0f43bc839b933",
      "d0978a3bfe884707870ed2eb7979adae",
      "501c106ad80c4243b30bd3dc14044ab3",
      "7094390be874465b916c32f1ba2ddc88",
      "229401f4d5cd435da548865db3d63709",
      "221cf0ad2300476790dc437fdafc738f",
      "3f6be899fe354003895fa130ece93143",
      "17f32d4b3d76436d8e69ea2befe090f9",
      "0a3b7f2c1bcf456a8b7fedb0ee81e326",
      "0c454de514914c2ea72be6efc5db9dec",
      "ff5d10d382c444faa4f7af4f86f38c5c",
      "a3cd0659135241b69cb80c16347c1c7d",
      "911d525b1c02451aa563d28f0bf88013",
      "bf96882f2b7743ecb61e5313fb779cff",
      "6f7b74f2d08f4331a17510040f4880b7",
      "58731fb7b85a43d99b9f59ba46f6d3ff",
      "699432adb93844b0a488271865ad6436",
      "63648842337245299b30809b97f20d01",
      "20ef3db4c8b44964812cae84d1968dd9",
      "96b9e3c9bddb4260abb6e8909f7579a1",
      "ce34a0eaba2540f5b2006f7022a6fdfe",
      "969b0ba46543481e9d6bb6207599dc00",
      "deda343f3b2543ab8ba2f0ca2190b30e",
      "27c84d066eed46b6b3822ba3a6f48e62",
      "200b493201804b56afee45374436790e",
      "0c9da30942b247d7a093ddb29862283f",
      "c80dfc98784e491ca7a153673bbca78b",
      "b9f4b7e674c54fd5b9895de29becacaa",
      "5be304753388424a96941a01614d2a5e",
      "f47e4f0921a84e6185256898f249b284",
      "2021ae2bb0744af5a243025c0afdda83",
      "72f6f13d46dc43e7ab6814f9bfbbfb40",
      "887188530482411daf0c7ae09f0e96f2",
      "1470dcbfcf4943a4b18b3050b8cf7060",
      "15df92b6742742beb58de7d7aae75c40",
      "da7029d7988e4a5590b5e9e08022a6bd",
      "55f20e081c1c408e922db52b5d1003d3",
      "1624e009151b4e7c994e064b1bda8701",
      "0790a1384d3d41418e6d392084f3c844",
      "a5ef52396ac5418fab96e14dd1d14712",
      "4ad77ccf488849f49c5694119998edac",
      "b4345bd2850b453ea67560f810f0d1f4",
      "5a849ba00e1b483abbc95f2abd588e48",
      "9a795eff863241fbb7a47e27f9abe1e9",
      "4f1dafce940b4d9c83ff6def83aa75d5",
      "e439383a3bbf46eb84bfbbfe824bb951",
      "2d54c8de014040be861ac50df3e099ff",
      "b9d9ccc20c8a42b893d562c3058c8b1c",
      "bba3910e0df64cb58d53408d26b134d4",
      "2ab0c623bca44daa8382a8244e99dc40",
      "6c6df08e38c2462c9cfdf53e55ead496",
      "67ab9fe0452c4385b2002adca5e19ca9",
      "8819b3de54df4dec8d743307f530e4db",
      "2957757f46204f2596c96281b31422e8",
      "aed31f843e5148498106fa2205318a83",
      "fd2a8ca6dc264029a6849b417eafddae",
      "907509685ca545379efaf4167d84f926",
      "fde35d017deb4b41a5bd928a05c403ca",
      "42839d0747e34ae1b79439be978e4f2d",
      "b7eb062b0306473d8e91455c0cf4ce0e",
      "987e9b273e0b4d06aad5c2a8323752d0",
      "95f3a8d64e284f99b6405845d8be1e71",
      "79d719ddbb07414cb99e06862594c478",
      "34808efbb9744433965206d2da8762d3",
      "03ccfe183fa2465c94e217d9c0cf028b",
      "a944254caa74430a8ea967ce1bc5d37c",
      "ad60f3988a5d435bb7d44ba606000e6c",
      "607e267d45cc48d2b065dd6e40e6991d",
      "570ab8ba9d4242b9a4a4f12218ca5d9e",
      "8bdc8c4154d048adb68b9ccbcae41753",
      "7bbb8b5fa79541a78700592780107fd2",
      "ff689ce62ba74ac5a9d4001ef1230d29",
      "aafbb48b5f92430783a245307c18e50d",
      "a5e7a5cca1804bee9b8f3e960422f65d",
      "f3a0f5784730423aa7b606ea5e80f64d",
      "bb6a22c6e0ec4c858a348e91504b2e12",
      "50d60d3229ce44859e61e18d51cead09",
      "d414fd017f844d5a84cf1227f78f8f19",
      "4a4a8384028241a9b3589f34b1945d7e",
      "ea2a621025204ff4baa9b1eeae2580d3",
      "37366c1478244ffa95f6a3e4223b1b59",
      "fbaa10e68a7349f793e095ee46588139",
      "3552bd730ef74a6cb3a7d88a2b3d1eaf",
      "a71c80c896b04ef8a11b4ccd0724223e",
      "2fa37004e1df4071858eb3a3e53224a0"
     ]
    },
    "id": "X9Wsb3xZzGeU",
    "outputId": "268d5a8d-1187-43ef-fb45-9b0c81fc41bf"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# ----------------------------\n",
    "# Load models\n",
    "# ----------------------------\n",
    "CAPTION_MODEL_ID = \"Salesforce/blip-image-captioning-base\"\n",
    "\n",
    "caption_processor = BlipProcessor.from_pretrained(CAPTION_MODEL_ID)\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(CAPTION_MODEL_ID)\n",
    "\n",
    "# ResNet-50 (as you already switched)\n",
    "clf_model = torch.hub.load(\"pytorch/vision:v0.6.0\", \"resnet50\", pretrained=True)\n",
    "clf_model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "caption_model = caption_model.to(device).eval()\n",
    "clf_model = clf_model.to(device).eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Labels\n",
    "# ----------------------------\n",
    "labels = requests.get(\"https://git.io/JJkYN\", timeout=30).text.strip().split(\"\\n\")\n",
    "if len(labels) < 1000:\n",
    "    raise RuntimeError(f\"Expected 1000 ImageNet labels, got {len(labels)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Classifier preprocessing (ImageNet standard)\n",
    "# ----------------------------\n",
    "clf_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def center_zoom_crop(pil_img: Image.Image, zoom: float) -> Image.Image:\n",
    "    \"\"\"Zoom > 1 crops tighter around center to bias classifier to closer object.\"\"\"\n",
    "    if zoom <= 1.0:\n",
    "        return pil_img\n",
    "    w, h = pil_img.size\n",
    "    new_w = max(1, int(w / zoom))\n",
    "    new_h = max(1, int(h / zoom))\n",
    "    left = (w - new_w) // 2\n",
    "    top = (h - new_h) // 2\n",
    "    return pil_img.crop((left, top, left + new_w, top + new_h))\n",
    "\n",
    "# ----------------------------\n",
    "# Lightweight realtime throttle cache (NEW)\n",
    "# ----------------------------\n",
    "_frame_count = 0\n",
    "_last_caption = \"\"\n",
    "_last_confidences = {}\n",
    "\n",
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def caption_and_classify(frame_rgb, zoom, topk, frame_stride, mode):\n",
    "    \"\"\"\n",
    "    frame_rgb: numpy array RGB from gr.Image(type=\"numpy\")\n",
    "    zoom: float\n",
    "    topk: int\n",
    "    frame_stride: int (run inference every N frames)\n",
    "    mode: \"Both\" | \"Caption only\" | \"Classify only\"\n",
    "    returns: caption(str), confidences(dict), status(str)\n",
    "    \"\"\"\n",
    "    global _frame_count, _last_caption, _last_confidences\n",
    "\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "\n",
    "        if frame_rgb is None:\n",
    "            return \"\", {}, \"No frame received yet.\"\n",
    "\n",
    "        # Throttle: only run heavy inference every N frames\n",
    "        _frame_count += 1\n",
    "        stride = max(1, int(frame_stride))\n",
    "        run_models = (_frame_count % stride == 0)\n",
    "\n",
    "        if not run_models:\n",
    "            ms = (time.time() - t0) * 1000\n",
    "            status = f\"OK (cached) | device={device} | stride={stride} | {ms:.0f} ms\"\n",
    "            # Return cached results\n",
    "            return _last_caption, _last_confidences, status\n",
    "\n",
    "        # Ensure uint8 for PIL\n",
    "        if frame_rgb.dtype != \"uint8\":\n",
    "            frame_rgb = frame_rgb.astype(\"uint8\")\n",
    "\n",
    "        img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "\n",
    "        caption = _last_caption\n",
    "        confidences = _last_confidences\n",
    "\n",
    "        # Enable autocast only on CUDA (NEW)\n",
    "        autocast_ctx = torch.cuda.amp.autocast if device == \"cuda\" else None\n",
    "\n",
    "        # ----- Caption on full frame -----\n",
    "        if mode in (\"Both\", \"Caption only\"):\n",
    "            cap_inputs = caption_processor(images=img, return_tensors=\"pt\")\n",
    "            cap_inputs = {k: v.to(device) for k, v in cap_inputs.items()}\n",
    "\n",
    "            if autocast_ctx:\n",
    "                with autocast_ctx():\n",
    "                    cap_out = caption_model.generate(\n",
    "                        **cap_inputs,\n",
    "                        max_new_tokens=25,  # slightly shorter for realtime\n",
    "                        num_beams=3,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "            else:\n",
    "                cap_out = caption_model.generate(\n",
    "                    **cap_inputs,\n",
    "                    max_new_tokens=25,\n",
    "                    num_beams=3,\n",
    "                    do_sample=False\n",
    "                )\n",
    "\n",
    "            caption = caption_processor.decode(cap_out[0], skip_special_tokens=True)\n",
    "\n",
    "        # ----- Classify on center-zoom crop -----\n",
    "        if mode in (\"Both\", \"Classify only\"):\n",
    "            cls_img = center_zoom_crop(img, float(zoom))\n",
    "            x = clf_transform(cls_img).unsqueeze(0).to(device)\n",
    "\n",
    "            if autocast_ctx:\n",
    "                with autocast_ctx():\n",
    "                    logits = clf_model(x)[0]\n",
    "            else:\n",
    "                logits = clf_model(x)[0]\n",
    "\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "            k = max(1, min(int(topk), 20))  # keep it sane for UI\n",
    "            vals, idxs = torch.topk(probs, k=k)\n",
    "\n",
    "            vals = vals.detach().cpu()\n",
    "            idxs = idxs.detach().cpu()\n",
    "            confidences = {labels[int(i)]: float(v) for v, i in zip(vals, idxs)}\n",
    "\n",
    "        # Update cache (NEW)\n",
    "        _last_caption = caption\n",
    "        _last_confidences = confidences\n",
    "\n",
    "        ms = (time.time() - t0) * 1000\n",
    "        status = f\"OK (inference) | device={device} | stride={stride} | {ms:.0f} ms\"\n",
    "        return caption, confidences, status\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"\", {}, f\"ERROR: {type(e).__name__}: {e}\"\n",
    "\n",
    "# ----------------------------\n",
    "# Gradio\n",
    "# ----------------------------\n",
    "demo = gr.Interface(\n",
    "    fn=caption_and_classify,\n",
    "    inputs=[\n",
    "        gr.Image(sources=[\"webcam\"], streaming=True, type=\"numpy\", label=\"Webcam\"),\n",
    "        gr.Slider(1.0, 3.0, value=1.8, step=0.1, label=\"Classifier center-zoom (higher = more foreground)\"),\n",
    "        gr.Slider(1, 10, value=5, step=1, label=\"Top-K classes\"),\n",
    "        gr.Slider(1, 6, value=2, step=1, label=\"Frame stride (run inference every N frames)\"),\n",
    "        gr.Radio([\"Both\", \"Caption only\", \"Classify only\"], value=\"Both\", label=\"Mode\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Caption (BLIP)\", lines=3),\n",
    "        gr.Label(label=\"Classification (ResNet50)\", num_top_classes=5),\n",
    "        gr.Textbox(label=\"Status / Debug\", lines=2),\n",
    "    ],\n",
    "    title=\"Realtime Webcam: Caption + Foreground-Biased Classification\",\n",
    "    description=(\n",
    "        \"Caption uses the full frame. Classification uses a center-zoom crop. \"\n",
    "        \"Use Frame stride to reduce load and improve realtime stability.\"\n",
    "    ),\n",
    "    live=True,\n",
    ")\n",
    "\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utWIB06hzKss"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
